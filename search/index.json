[{"content":"Hadoop Cluster\rRockyLinux9.3 \u0026amp;\u0026amp; Docker \u0026amp;\u0026amp; JDK8 \u0026amp;\u0026amp; Hadoop3.2.4 (jdk21 is NOT compatible with hadoop3.4.0)\nBasic Operations If Needed\rmapping\rvim /etc/hosts append \u0026raquo; \u0026ldquo;192.168.2.4 hostname\u0026rdquo;\nclose the firewall\rsystemctl stop firewalld systemctl disable firewalld handle JDK and hadoop env variable\rin /etc/profile or ~/.bashrc:\n1 2 3 4 5 6 export JAVA_HOME=/root/Downloads/jdk1.8.0_181 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export HADOOP_HOME=/root/Downloads/hadoop-3.2.4 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin Docker Image with hadoop installed\r/path/hadoop-3.2.4/etc/hadoop\rhadoop-env.sh\rexport JAVA_HOME=/root/Downloads/jdk1.8.0_181 core-site.xml\r1 2 3 4 5 6 7 8 9 10 11 \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:/usr/local/hadoop/tmp\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Abase for other temporary directories.\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://master:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; hdfs-site.xml\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:/usr/local/hadoop/namenode_dir\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.datanode.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:/usr/local/hadoop/datanode_dir\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; mapred-site.xml\r1 2 3 4 5 6 \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; yarn-site.xml\r1 2 3 4 5 6 7 8 9 10 11 \u0026lt;configuration\u0026gt; \u0026lt;!-- Site specific YARN configuration properties --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.hostname\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; /path/hadoop-3.2.4/sbin if root needed\rstart-all.sh \u0026amp;\u0026amp; stop-all.sh (must be in the front the file)\rexport HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root ssh config if needed\rcreate public/private keys\nssh-keygen -t rsa create authorized_keys and change mod\ncd ~/.ssh touch authorized_keys chmod 600 authorized_keys add public key into authorized_keys\ncat id_rsa.pub \u0026raquo; authorized_keys create image\rdocker commit [container_id/name] [image_name] Config master and slaves\rcreate containers:\rdocker run -it [-v ~/hadoopMapping/hosts:/etc/hosts] -h master \u0026ndash;name master ubuntu/hadoopinstalled\ndocker run -it [-v ~/hadoopMapping/hosts:/etc/hosts] -h slave01 \u0026ndash;name slave01 ubuntu/hadoopinstalled\ndocker run -it [-v ~/hadoopMapping/hosts:/etc/hosts] -h slave02 \u0026ndash;name slave02 ubuntu/hadoopinstalled\nIf needed(hostname \u0026lt;==\u0026gt; ip address), [-v path/hosts:/etc/hosts] or create new docker network.\nadd slave computer\rvim /path/hadoop/etc/hadoop/workers,\nreplace localhost with the hostnames of two slaves.\nFormat and Start Hadoop\rformat\rhadoop namenode -format start \u0026amp;\u0026amp; stop hadoop\rstart-all.sh stop-all.sh use hadoop\rlook through the hadoop process\njps look through port service\nnetstat -tpnl | grep java test:\nbrower \u0026raquo; http://ip:8088\nterminal \u0026raquo; curl http://ip:8088/cluster\nReference\rblog\nBasic Operation\rread folder and file information\rhdfs dfs -ls [-R] [-h] / recursive human-readable create/remove file/folder\rhdfs dfs -mkdir -p /user/root hdfs dfs -touchz /path/to/hdfs/emptyfile hdfs dfs -rm [-rf] /path/to/hdfs/directory_or_file judge existence and look through\rhdfs dfs -test -e /user/root/demo.txt echo $? hdfs dfs -cat /user/root/demo.txt upload \u0026amp;\u0026amp; download \u0026amp;\u0026amp; append file\rhdfs dfs -put [-f] demo.txt /user/root hdfs dfs -get demo.txt /root/ hdfs dfs -appendToFile /root/demo.txt /user/root/demo.txt hdfs dfs -copyFromLocal [-f] /root/demo.txt /user/root/demo.txt move or rename file\rhdfs dfs -mv test.txt dir/test.txt Hive Preparation\rhadoop3.2.4 has already been installed successfully downloaded hive 3.1.2 and mysql jdbc pull mysql image and run it docker run -p 3306:3306 \u0026ndash;name mysql -v /data:/data -e MYSQL_ROOT_PASSWORD=123123 \u0026ndash;privileged=true [images_name] Config Mysql\rinstall mysql client\rapt-get update apt-cache policy mysql-client apt-get install mysql-client=8.0.?-?ubuntu? mysql -h ?ip -P 3306 -u myuser -p operate database\rmysql\u0026gt; create database hive; mysql\u0026gt; flush privileges; Config Hive\runcompose hive.tgz file\rmv mysql-connector-java-8.0.28.tar ~~/hive/lib\nremove conflicting jar files if needed\rdelete ~~/hive/lib/log4j-slf4j-*-2.10.0.jar\ncopy and paste higher version\r~~/hadoop/share/hadoop/common/lib/guava*.jar\n~~/hive/lib/guava*.jar\nin ~~/hive/conf\rmv hive-default.xml.template hive-default.xml vim hive-site.xml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; standalone=\u0026#34;no\u0026#34;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://??????????:3306/hive(db_name created)?createDatabaseIfNotExist=true\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;JDBC connect string for a JDBC metastore\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Driver class name for a JDBC metastore\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;root\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;!!!username to use against metastore database\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;123123\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;!!!password to use against metastore database\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.active.passive.ha.enable\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; Initialize and start\rschematool -dbType mysql -initSchema -verbose\nhive\nshow databases;\nRemote Connection\rvim ~~/hadoop/etc/hadoop/core-site.xml\nadd:\n1 2 3 4 5 6 7 8 9 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.root.hosts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.root.groups\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; start server:\nhiveserver2 Hbase Preparation\rHadoop has already been installed successfully. JDK8 \u0026amp;\u0026amp; hadoop3.2.4 \u0026amp;\u0026amp; hbase2.4.17 \u0026amp;\u0026amp; zookeeper3.7.2 Zookeeper Config\rDownload and install zookeeper in /usr/local(recommended) Run command: mv zoo_sample.cfg zoo.cfg in zookeeper/conf Modify zoo.cfg 1 2 3 4 5 6 dataDir=/usr/local/zookeeper/data dataLogDir=/usr/local/zookeeper/log server.1=master:2888:3888 server.2=slave01:2888:3888 server.3=slave02:2888:3888 Copy the current folder and paste it to the other hosts\nIn the ~~/zookeeper/data of each computer, create a file named \u0026lsquo;myid\u0026rsquo; and write 1, 2, 3 in each respectively\nStart each PC respectively\n/usr/local/zookeeper/bin/zkServer.sh start ​Look through the status\n/usr/local/zookeeper/bin/zkServer.sh status HBase Config\rhbase-site.xml\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.rootdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://master:9000/hbase\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.cluster.distributed\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;!--false--\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.master.port\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;16000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.master.info.port\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;60010\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.quorum\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master,slave01,slave02\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.property.clientPort\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2181\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.property.dataDir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/usr/local/zookeeper/data\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;./tmp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.unsafe.stream.capability.enforce\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; hbase-env.sh\r1 2 3 4 export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export HADOOP_HOME=/usr/local/hadoop export HBASE_HOME=/usr/local/hbase export HBASE_MANAGES_ZK=false and in the last,\nuncommit:\nexport HBASE_DISABLE_HADOOP_CLASSPATH_LOOKUP=\u0026ldquo;true\u0026rdquo; regionservers\r1 2 slave01 slave02 Copy the hbase folder and paste it to the other hosts.\nStart HBase\rstart zookeeper first Each computer needs to be started.\nstart hdfs(hadoop) then\nstart hbase\n/usr/local/hbase/bin/start-hbase.sh Deploy\rhbase thrift -p 9090 start python connection\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import happybase try: # Connect to HBase connection = happybase.Connection(\u0026#39;172.17.0.2\u0026#39;, port=9090) # Get the names of all tables tables = connection.tables() # Print the names of all tables print(\u0026#34;Names of all tables:\u0026#34;) for table_name in tables: print(table_name.decode(\u0026#39;utf-8\u0026#39;)) # Close the connection connection.close() except Exception as e: print(f\u0026#34;Error connecting to HBase: {e}\u0026#34;) Reference\rInfo\nspark preparation\rhadoop3.2.4 spark3.4.3-bin-without-hadoop.tgz Procedure\rvim ~~/spark/conf/workers\nvim ~~/spark/conf/spark-env.sh\nadd \u0026raquo;\n1 2 3 4 export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop-3.2.4/bin/hadoop classpath) export HADOOP_CONF_DIR=/usr/local/hadoop-3.2.4/etc/hadoop export SPARK_MASTER_HOST=master export JAVA_HOME=/usr/local/java8 Test\rcluster \u0026raquo; standalone\nspark-submit \u0026ndash;master spark://master:7077 /usr/local/- spark/examples/src/main/python/pi.py 2\u0026gt;\u0026amp;1 | grep \u0026ldquo;Pi is roughly\u0026rdquo; cluster \u0026raquo; yarn\nspark-submit \u0026ndash;master yarn /usr/local/spark/examples/src/main/python/pi.py 2\u0026gt;\u0026amp;1 | grep \u0026ldquo;Pi is roughly\u0026rdquo; ","date":"2024-04-27T00:00:00Z","permalink":"https://charlesyifanli.github.io/p/hadoop/","title":"hadoop"},{"content":"install docker in ubuntu\rsudo apt update sudo apt install docker.io docker-compose [ sudo usermod -aG docker ${USER_NAME} ] docker mirror\rsudo cd /etc/docker vim daemon.json (if no, create) add: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker.registry.cyou\u0026#34;, \u0026#34;https://docker-cf.registry.cyou\u0026#34;, \u0026#34;https://dockercf.jsdelivr.fyi\u0026#34;, \u0026#34;https://docker.jsdelivr.fyi\u0026#34;, \u0026#34;https://dockertest.jsdelivr.fyi\u0026#34;, \u0026#34;https://mirror.aliyuncs.com\u0026#34;, \u0026#34;https://dockerproxy.com\u0026#34;, \u0026#34;https://mirror.baidubce.com\u0026#34;, \u0026#34;https://docker.m.daocloud.io\u0026#34;, \u0026#34;https://docker.nju.edu.cn\u0026#34;, \u0026#34;https://docker.mirrors.sjtug.sjtu.edu.cn\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://mirror.iscas.ac.cn\u0026#34;, \u0026#34;https://docker.rainbond.cc\u0026#34; ] } basic operation\rcreate \u0026amp;\u0026amp; query \u0026amp;\u0026amp; delete image\rdocker build -t repo_name:tag . docker images docker rmi repo_name:tag/id run \u0026amp;\u0026amp; query \u0026amp;\u0026amp; delete container\rdocker run [-it/-d] [-p host_port:container_port] [-v /host/data:/container/data] \u0026ndash;name container_name \u0026ndash;hostname hostname repo_name:tag\n-it: interact with current console -d: detach -p: set port -v: set volume docker ps [-a]\ndocker rm container_name/id\nstart \u0026amp;\u0026amp; stop container\rdocker start/stop container_name docker stop $(docker ps -q) //stop all container docker exec -it container_name bash docker attach container_name \u0026ldquo;Ctrl + P + Q\u0026rdquo; [interact with current console] query container volume\rdocker inspect \u0026ndash;format=\u0026rsquo;{{json .Mounts}}\u0026rsquo; \u0026lt;container_name or container_id\u0026gt; copy during hosts and containers\rdocker cp /root/build/demo.txt master:/root/build docker cp master:/root/build/demo.txt /root/build Docker and Github Package\rpreparation\rgithub \u0026raquo; settings \u0026raquo; developer settings \u0026raquo; get the token with privilege \u0026ldquo;write packages\u0026rdquo;\noperations\rdocker login\rdocker login ghcr.io -u [github name] -p [token] image format\rghcr.io/[github name]/repo_name/image_name:tag # [full_image_name == repo_name/image_name] push \u0026amp;\u0026amp; pull\rdocker push ghcr.io/[github name]/repo_name/image_name:v1.2 # [full_image_name == repo_name/image_name] docker pull ghcr.io/[github name]/repo_name/image_name:v1.2 # [full_image_name == repo_name/image_name] exit\rdocker logout Docker Network\rlist docker network\rdocker network list inspect\rdocker network inspect [name] | less view the IP address information on the network device veth_name, output it in JSON format, and use jq to format and process the JSON data.\rip -json address show dev [veth_name] | jq Reference\rInfo\nNetwork\n","date":"2024-04-26T00:00:00Z","permalink":"https://charlesyifanli.github.io/p/docker/","title":"docker"},{"content":"SpingBoot3\rhot deployment\radd maven dependencies: 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-devtools\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; idea =\u0026gt; settings =\u0026gt; Build =\u0026gt; Compiler allow \u0026ldquo;Build Project Automatically\u0026rdquo;\nidea =\u0026gt; settings =\u0026gt;Advanced Settings =\u0026gt; Compiler allow auto-make to start even if developed application is currently running\n","date":"2024-04-16T00:00:00Z","permalink":"https://charlesyifanli.github.io/p/springboot/","title":"springboot"},{"content":"Set terminal shortcuts\r1 click on \u0026#34;settings\u0026#34; 1 click on \u0026#34;Customize Shortscuts\u0026#34; in Keyboard Shortcuts 1 click on \u0026#34;Custom Shortcuts\u0026#34; 1 2 3 name: terminal Command: /usr/bin/gnome-terminal Shortcut: Ctrl + Alt + L Set Vim\rcreate ~/.vimrc\nset nu . ~/.vimrc Set row number\r:set nu :set number Enter Insert Mode:\ri: Insert text before the cursor. a: Insert text after the cursor. o: Open a new line below the current line and switch to insert mode. I: Insert text before the cursor on the current line. A: Append text after the cursor on the current line. O: Open a new line above the current line and switch to insert mode. Exit Insert Mode:\rEsc: Exit the current insert mode. Save File:\r:w: Save the file without exiting Vim. :wq or :x: Save the file and quit Vim. Quit Without Saving:\r:q!: Quit without saving changes. Move Cursor:\rh: Move left one character. j: Move down one line. k: Move up one line. l: Move right one character. gg: Go to the beginning of the file. G: Go to the end of the file. Copy, Cut, and Paste:\ryy: Copy the current line. dd: Cut the current line. p: Paste the contents of the clipboard after the current position. Undo and Redo:\ru: Undo the last operation. Ctrl + r: Redo the last undone operation. Search and Replace:\r/keyword: Search for the specified keyword. :%s/old/new/g: Replace all occurrences of old with new throughout the file. It is recommended to use vim to edit something and use less to look through the text.\nBlock device (USB)\rCommand line:\rlsblk mount /dev/sdb1 ./usb/ umount ./usb If user-oriented, run fowllowing commands in order:\nmount /dev/sdb1 -o uid=? gid=? /dev/sdb1 ./usb/ umount ./usb Explanation\r1 2 3 4 5 6 7 sda: The naming convention for storage devices in Linux, such as hard drives, solid-state drives, and USB drives, uses \u0026#34;sda\u0026#34; and \u0026#34;sdb\u0026#34;. This convention consists of the following parts: \u0026#34;sd\u0026#34;: Represents SCSI devices. Although called SCSI devices, they are not necessarily true SCSI devices but rather follow a naming convention. \u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, etc.: Represents the device\u0026#39;s identifier, starting from \u0026#34;a\u0026#34; and incrementing. Therefore, \u0026#34;sda\u0026#34; represents the first SCSI device in the system, typically the primary hard drive, while \u0026#34;sdb\u0026#34; represents the second SCSI device, typically another hard drive or external device like a USB drive. Following this pattern, \u0026#34;sdc\u0026#34;, \u0026#34;sdd\u0026#34;, and so on represent the third, fourth, and subsequent SCSI devices in the system, respectively. 1 2 3 4 5 6 7 sr0: In the Linux system, \u0026#34;sr0\u0026#34; is a naming convention used to represent optical disc drives. It typically denotes the first optical disc drive in the system, where: - \u0026#34;sr\u0026#34;: Represents SCSI CD-ROM devices. Despite being referred to as SCSI devices, they may not necessarily be true SCSI devices but rather follow a naming convention. - \u0026#34;0\u0026#34;: Represents the device\u0026#39;s identifier, starting from 0. Therefore, \u0026#34;sr0\u0026#34; signifies the first CD-ROM or DVD-ROM drive in the system. 1 2 3 4 5 6 7 8 9 nvme0n1p1: nvme0n1 is a naming convention used in the Linux system to represent NVMe (Non-Volatile Memory Express) devices. NVMe is an interface and protocol designed for connecting high-performance solid-state drives (SSDs). nvme0n1 denotes the first NVMe device in the system, where: - nvme: Represents NVMe devices. - 0: Represents the device\u0026#39;s identifier, starting from 0. - n1: Represents the first namespace (or partition) within the device. - p1: Represents the first partition within the device. Therefore, nvme0n1p1 represents the first partition of the first NVMe device in the system (typically an NVMe SSD). If there are multiple NVMe devices in the system, they might be named nvme0n1, nvme1n1, nvme2n1, and so on, following a similar pattern. If the devices have multiple partitions, they might be named nvme0n1p2, nvme0n1p3, and so forth. Package manager (fedora)\rsuggested downloading and installing approach\r1 2 3 dnf config-manager --add-repo URL+[name].repo dnf install [name] [systemctl enable --now [name]] Or\nUse wget to download the .rpm file to the system (or by browser or USB drive): wget [-O /path/to/destination/package.rpm] https://example.com/package.rpm Use the dnf command to install the .rpm file: dnf install package.rpm dnf will handle dependency resolution during the installation process and retrieve any necessary dependencies from configured software sources. Therefore, this is a convenient method to install .rpm files and their dependencies.\nrpm command\rrpm -qa | grep name rpm\rrpm (Red Hat Package Manager) is a low-level tool used to install, upgrade, and remove software packages on RPM-based Linux systems. RPM operates directly on local .rpm files but does not involve package downloading or dependency resolution. It is primarily used for direct management of local software packages, requiring manual resolution of package dependencies.\nyum\ryum (Yellowdog Updater Modified) is an advanced package management tool built on top of rpm. It is an automated package manager capable of handling package downloads, dependency resolution, installation, updates, and removal. yum utilizes pre-configured software repositories to obtain packages and can automatically resolve dependencies between packages, making package management more convenient and efficient.\ndnf\rdnf (Dandified Yum) is the successor to yum, designed to be a replacement for yum. dnf uses different dependency resolution algorithms and has made improvements in performance, stability, and functionality compared to yum. It provides faster package management capabilities and is easier to use.\ncomparation\rrpm \u0026amp; yum\rCompared to rpm, yum has an additional feature of automatically resolving dependencies. The method yum uses to resolve dependencies is by automatically downloading them.\nrpm \u0026amp; wget\rWget is a command-line utility used to retrieve files from web servers using HTTP, HTTPS, FTP, and FTPS protocols. It is commonly used to download files, including software packages, from the internet to a local system. Environment variables\rrecommended (for instance):\rvim /etc/profile\nappend \u0026raquo; {\nexport JAVA_HOME=/root/Downloads/jdk1.8.0_181 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\nexport HADOOP_HOME=/root/Downloads/hadoop-3.2.4 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n}\n~/.bashrc \u0026amp;\u0026amp; /etc/profile\rtouch ~/.bash_profile load ~/.bash_profile into ~/.bashrc 1 2 3 4 5 `.bashrc` is a user-level configuration file that only affects the Bash environment of the current user. `/etc/profile` is a system-level configuration file that affects the Bash environment for all users. `.bashrc` is executed when a user starts an interactive Bash shell, whereas `/etc/profile` is executed when a user logs in. Users can customize their settings in `.bashrc`, while configurations in `/etc/profile` are typically managed by system administrators for consistency across all users. Copy-paste\rcp source_file(folder) destination_folder [-r] mv source_file(folder) destination_folder mv demo.txt test.txt //rename the file rm file(folder)_name [-r -f -v -i] Compress \u0026amp; decompress\rbasic infos\r1 2 3 4 5 6 .tar VS .tar.gz VS .tgz \u0026gt;\u0026gt; \u0026#34;.tar\u0026#34;: This is the most basic archive file format, which bundles multiple files or directories into a single file without compression. Therefore, .tar files are commonly referred to as tarballs or tar archives. \u0026#34;.tar.gz\u0026#34;: This is a file format compressed using the gzip compression algorithm on top of the .tar format. It bundles multiple files or directories into a .tar file, which is then compressed using gzip to create a .tar.gz file. Hence, .tar.gz and .tgz are the same format, with different file extensions, both representing a file that has been bundled with tar and compressed with gzip. compress\rUsing tar and gzip:\rtar -czvf ./archive.tar.gz /path/to/directory_or_file -c: Create a compressed file. -z: Use gzip compression. -v: Display detailed output. -f: Specify the name of the compressed file. archive.tar.gz: The name of the compressed file. /path/to/directory_or_file: The directory or file to be compressed. Using zip:\rzip -r ./archive.zip /path/to/directory_or_file -r: Recursively compress directories and their contents. archive.zip: The name of the compressed file. /path/to/directory_or_file: The directory or file to be compressed. decompress\rUsing tar and gzip:\rtar -xzvf ./archive.tar.gz [-C /output] -x: Extract files. -z: Use gzip decompression. -v: Display detailed output. -f: Specify the file to be decompressed. archive.tar.gz: The name of the file to be decompressed. Using unzip:\runzip ./archive.zip [-d /output] archive.zip: The name of the file to be decompressed. Unbind process from terminal\rhang up\rThis method puts the command in the background, but if the terminal is closed, the command may receive the SIGHUP signal and be terminated.\n[your command] \u0026amp; no hang up\rUsing the nohup command redirects the output of the command to the file nohup.out, so even if the terminal is closed, the execution of the command will not be terminated.\nnohup [your command] \u0026amp; however,\rWhen using the nohup command to run a process in the background, the nohup.out file is created by default in the current working directory. It is used to store the standard output and standard error messages of the process started by the nohup command.\nselect output path\rnohup [your command] \u0026gt; /path \u0026amp; discard the nohup.out file\rnohup [your command] \u0026gt; /dev/null \u0026amp; process status\rView real-time system process information\rtop Display the processes:\ronly current user all users detailed process information a tree-like process list a specific process\nps ps -e ps aux (ps aux | grep \u0026lt;process_name\u0026gt;) ps auxf ps -e | grep process_name Close process\rkill pid Remote connections\rfedora\rIf you want to allow the root user to log in via SSH using a password, you can modify the PermitRootLogin parameter in the /etc/ssh/sshd_config file. Set it to yes to allow the root user to log in via SSH using a password.\nFollow these steps to make the changes:\nOpen a terminal and log in to your server as the root user.\nUse a text editor such as nano, vim, etc., to open the /etc/ssh/sshd_config file.\nFind the PermitRootLogin line and modify it to:\n1 PermitRootLogin yes if no sshd_config, install oepnssh-server first\nSave the file and exit the editor.\nReload the SSH service to apply the changes. You can do this by running the following command:\n1 systemctl reload sshd ubuntu\rupdate password\nsudo passwd root modify pam.d/\ncd /etc/pam.d sudo gedit gdm-autologin sudo gedit gdm-passwd 1 2 make: #auth required pam_succeed_if.so user != root quiet_success modify ~/.profile\n1 2 3 make: #mesg n 2\u0026gt; /dev/null || true tty -s \u0026amp;\u0026amp; mesg n || true then\nreboot File permissions\rcommand\rEach permission is represented by a number, ranging from 0 to 7, which represents the combination of read (4), write (2), and execute (1) permissions.\nchmod 755 file.txt info\rOwner Permissions: Permissions held by the creator of the file or directory, controlling ownership and permission settings for the file.\nGroup Permissions: Specifies the permissions for the user group to which the file belongs. Besides the owner, a group of users is granted the same set of permissions.\nOthers Permissions: Specifies permissions for all other users.\nThese permissions are often represented by a string of characters, such as rwxr-xr-x, where:\nThe first character represents the file type (- indicates a regular file, d indicates a directory, and so on). Nvidia Driver and Cuda Config in ubuntu\rmethod01（recommended）\rlook through suitable version\rsudo ubuntu-drivers devices search for what like \u0026ldquo;\u0026hellip;\u0026hellip;..non free recommended\u0026rdquo;\nclick: software update \u0026raquo; additional drivers\rselect recommended driver\nmethod02\rwhat kind of gpu\rlspci | grep -i vga driver\r[ sudo apt remove \u0026ndash;purge \u0026lsquo;^nvidia-.*\u0026rsquo; ] [ sudo apt remove \u0026ndash;purge \u0026lsquo;^libnvidia-.*\u0026rsquo; ] [ dpkg -l | grep nvidia ] sudo apt install nvidia-driver-550 sudo apt install nvidia-utils-550 nvidia-smi cuda\rsudo apt install nvidia-cuda-toolkit reference\nUbuntu Installation with Intel RST closed during double systems\renter windows system\rwin + R input \u0026raquo; msconfig select Boot, click \u0026ldquo;Safe boot\u0026rdquo; in Boot options restart enter bios\rchange \u0026ldquo;SATA\u0026rdquo; mode to AHCI\nenter windows system safe mode\rwin + R input \u0026raquo; msconfig select Boot, cancel \u0026ldquo;Safe boot\u0026rdquo; in Boot options restart install ubuntu normally\rs\n","date":"2024-03-08T00:00:00Z","permalink":"https://charlesyifanli.github.io/p/linux/","title":"linux"},{"content":"conda config\rview mirror sources\rconda config \u0026ndash;show-sources display config file\rconda config \u0026ndash;set show_channel_urls yes update ~/.condarc\nexample\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 channels: - defaults default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud ssl_verify: true show_channel_urls: true auto_activate_base: false create and delete envs\rconda create -n \u0026lt;env_name\u0026gt; [python=3.10] conda remove -n \u0026lt;environment_name\u0026gt; \u0026ndash;all clear cache index\rconda clean -i pip config\rview mirror sources\rpip config list pip command line\nmethods\rwindows\rnew ~/pip/pip.init update pip.ini linux\rnew ~/.config/pip/pip.conf update pip.conf example\r1 2 3 4 5 6 7 8 [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple trusted-host = pypi.tuna.tsinghua.edu.cn [global] index-url = https://mirror.nju.edu.cn/pypi/web/simple [install] trusted-host = https://mirror.nju.edu.cn/pypi/web reference\rUnited Mirrors\nTsinghua\nInfo\nLinux\nJupyer\n","date":"2024-02-08T00:00:00Z","permalink":"https://charlesyifanli.github.io/p/conda_pip_source/","title":"conda_pip_source"},{"content":"Close delivery optimization\rpath\rsettings -\u0026gt; Update\u0026amp;Security -\u0026gt; delivery optimization\neffect\rAutomatic Source Selection Privacy Concerns\nLAN\rpc shared with\rSelect the folder you want to share, right-click and choose Properties, click on Sharing, Advanced Sharing, Share this folder, Permissions, select Full Control. Confirm. Choose Security, Edit, Add, Input Everyone, save and close. Right-click Network, click on Change advanced sharing settings, All networks, turn off password-protected sharing. other PC in LAN\rIn file explorer: Input: \\ip_address Input: Everyone\nhow to use remote desktop\rpath\rSettings =\u0026gt; System =\u0026gt; About =\u0026gt; Advanced system settings =\u0026gt; Remote\noperation\rRemote Assistance \u0026raquo; both are right Remote Desktop \u0026raquo; Only \u0026ldquo;Allow remote connections to this computer\u0026rdquo;(No sub item)\nset remote desktop user\rright click win icon \u0026raquo; computer management =\u0026gt; Local Users and Groups =\u0026gt; Users create a new user or use an existed user(recommended: password never expires), double click Member of =\u0026gt; Add =\u0026gt; write \u0026ldquo;remote desktop users\u0026rdquo; =\u0026gt; Check Names =\u0026gt;ok\nwin+r\rmstsc =\u0026gt; show options =\u0026gt; ip + username (of controllered pc)\nactivate conda\ruse conda scripts in powershell\rget-ExecutionPolicy set-ExecutionPolicy RemoteSigned More info: reference\ninit powershell\rconda init powershell The profile.ps1 file is generated by default in the C:\\Users\\charles\\Documents\\WindowsPowerShell directory.\ndeactivate env name automatically\rconda config \u0026ndash;set auto_activate_base false More info: reference\ncharacter setting\rdisplay the code page number\rchcp set the console code page to UTF-8 temporarily\rchcp 65001 count sub directories in powershell\ronly current sub directories\r(Get-ChildItem -Directory | Where-Object { $_.FullName -match \u0026lsquo;\\[^\\]+\\[^\\]+$\u0026rsquo; }).Count all sub directories\r(Get-ChildItem -Directory -Recurse | Where-Object { $_.FullName -match \u0026lsquo;\\[^\\]+\\[^\\]+$\u0026rsquo; }).Count\nen-win11, garbled issue in Chinese software and drivers\rcontrol panel clock and region Region (Change date, time, or number formats) Administrative Change system locale Blog - hexo\rlocal\rThe default GitHub-related configurations have been completed. npm install -g hexo-cli\nhexo init blog_name\ncd blog_name\nnpm install (only download in current dir)\nhexo cl\nhexo g\nhexo s\nmodify \u0026ldquo;_config.yml\u0026rdquo;: 1 2 3 4 5 ...... deploy: type: git repo: \u0026lt;github url\u0026gt; branch: \u0026lt;branch-name\u0026gt; command line npm install hexo-deployer-git \u0026ndash;save\nhexo cl\nhexo g\nhexo d\nnote\rnode_modules: Dependency packages public: Storage for generated pages scaffolds: Templates for generating articles source: Location for storing your articles themes: Themes _config.yml: Configuration file for the blog tip\rEvery time \u0026lsquo;hexo d\u0026rsquo; command is executed, it uploads the contents generated locally inside the \u0026lsquo;.deploy_git\u0026rsquo; directory. Other files, including those within the \u0026lsquo;source\u0026rsquo; directory, configuration files, and theme files, are not uploaded to GitHub. Without the source files, it\u0026rsquo;s impossible to regenerate the pages.\nInfo: reference\ntheme configuration: voluntis\rThanks to source code and crash course!\nBy the way: voluntis documentation.\nBlog - hugo\rreference\npowershell theme config: oh-my-posh\rinit oh-my-posh\rnotepad $profile add context: oh-my-posh init pwsh | Invoke-Expression\nswitch theme\rGet-PoshThemes\nnotepad $profile\nmodify context: oh-my-posh init pwsh \u0026ndash;config \u0026lt;\\path\u0026gt; \\ \u0026lt;\\name\u0026gt;.omp.json | Invoke-Expression\nallow to display conda env\rget theme path\rGet-PoshThemes modify the theme json file, in \u0026ldquo;blocks\u0026rdquo; in \u0026ldquo;segments\u0026rdquo;, add:\r1 2 3 4 5 6 7 8 9 10 11 12 13 { \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;style\u0026#34;: \u0026#34;powerline\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;display_mode\u0026#34;: \u0026#34;environment\u0026#34;, \u0026#34;fetch_virtual_env\u0026#34;: true, \u0026#34;home_enabled\u0026#34;: true }, \u0026#34;powerline_symbol\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;foreground\u0026#34;: \u0026#34;#100e23\u0026#34;, \u0026#34;background\u0026#34;: \u0026#34;#906cff\u0026#34;, \u0026#34;template\u0026#34;: \u0026#34;  {{ if .Error }}{{ .Error }}{{ else }}{{ if .Venv }}{{ .Venv }} {{ end }}{{ .Full }}{{ end }} \u0026#34; }, More info: reference\n","date":"2023-12-11T00:00:00Z","permalink":"https://charlesyifanli.github.io/p/windows/","title":"Windows"},{"content":"change npm registry\rnpm config get registry\nnpm config set registry https://registry.npmmirror.com/\nnpm config set registry https://registry.npmjs.org/\nnvm intro\rwhat is \u0026ldquo;nvm\u0026rdquo;\rNVM (Node Version Manager) is a tool that manages multiple Node.js versions.\ncommon command\rnvm ls - View currently installed versions\nnvm install nvm use nvm list available - Display a partial list of available versions for download\nnvm uninstall nvm alias - Add aliases to different version numbers\nnvm unalias - Remove defined aliases\nnvm reinstall-packages - Reinstall globally installed npm packages for a specified version in the current Node environment\nMore info: reference\n","date":"2023-12-10T00:00:00Z","permalink":"https://charlesyifanli.github.io/p/javascript/","title":"javascript"},{"content":"git branch intro\rThe main branch is responsible for merging other branches, while other branches are responsible for completing their respective tasks and continuing iterations.\nSolution: \u0026ldquo;A repository corresponds to a sub-branch, create a privileged repository corresponding to all branches, pull all branches in the privileged repository first, then perform a --no-ff merge of other branches into the main branch, and push.\u0026rdquo;\nWorkflow: reference\ncommand: git merge \u0026ndash;no-ff \u0026ldquo;branch-name\u0026rdquo;\rBefore merge:\r1 2 3 o---o---o---o master \\ o---o feature Merge with Default (Fast-forward Enabled):\r1 2 3 o---o---o---o---o master (fast-forwarded to feature) \\ o---o feature Merge with \u0026ndash;no-ff Flag:\r1 2 3 o---o---o---o-------o master \\ / o---o---o feature Note\rWhen merging without the --no-ff flag, the master branch pointer moves directly to the tip of the feature branch. This process doesn\u0026rsquo;t show a distinct merge operation in the commit history.\nHowever, when merging using the --no-ff flag, a new merge commit is generated, preserving the separate histories of both master and feature branches.\nMore info: reference\nssh and github\rgenerate key\rssh-keygen -t rsa -C \u0026ldquo;xxx@xxx.com\u0026rdquo; get public key\rcd ~/.ssh cat id_rsa.pub add public key into github and test\rssh -T git@github.com ssh: connect to host github.com port 22: Connection timed out\rIn \u0026ldquo;~/.ssh/config\u0026rdquo;,\r1 2 3 Host github.com Hostname ssh.github.com Port 443 More info: reference\nbranch based on a historical version\rClone the repository locally:\rgit clone cd your-repository Create a new branch\rgit checkout -b new-branch-name Look through the hash value of specific history version\rgit log Reset the branch\rgit reset \u0026ndash;hard commit-hash Set upstream\rgit push \u0026ndash;set-upstream origin branch-name Merge branch\rgit switch name git merge name_to_be_merged Delete branch\rgit branch -d name_to_be_delete Delete remote corresponding brach\rgit add .;git commit -m *;git push git push origin \u0026ndash;delete name_to_be_delete ","date":"2023-12-07T00:00:00Z","permalink":"https://charlesyifanli.github.io/p/git_config/","title":"git_config"}]